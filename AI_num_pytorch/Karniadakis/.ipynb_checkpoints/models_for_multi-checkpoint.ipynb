{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model using high-fidelity alone\n",
    "## \n",
    "class DNN_HF(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(DNN_HF, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        T.nn.init.xavier_uniform_(self.linear1.weight,gain = 0.0)\n",
    "        T.nn.init.zeros_(self.linear1.bias)\n",
    "        \n",
    "        self.linear2 = torch.nn.Linear(H, H)\n",
    "        T.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        T.nn.init.zeros_(self.linear2.bias)\n",
    "        \n",
    "        self.linear3 = torch.nn.Linear(H, H)\n",
    "        T.nn.init.xavier_uniform_(self.linear3.weight)\n",
    "        T.nn.init.zeros_(self.linear3.bias)\n",
    "        \n",
    "        self.linear4 = torch.nn.Linear(H, H)\n",
    "        T.nn.init.xavier_uniform_(self.linear4.weight)\n",
    "        T.nn.init.zeros_(self.linear4.bias)\n",
    "        \n",
    "        self.linear5 = torch.nn.Linear(H, H)\n",
    "        T.nn.init.xavier_uniform_(self.linear5.weight)\n",
    "        T.nn.init.zeros_(self.linear5.bias)\n",
    "        \n",
    "        self.linear6 = torch.nn.Linear(H, D_out)\n",
    "        T.nn.init.xavier_uniform_(self.linear6.weight)\n",
    "        T.nn.init.zeros_(self.linear6.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = torch.tanh(self.linear1(x))\n",
    "        h_relu = torch.tanh(self.linear2(h_relu))\n",
    "        h_relu = torch.tanh(self.linear3(h_relu))\n",
    "        h_relu = torch.tanh(self.linear4(h_relu))\n",
    "        h_relu = torch.tanh(self.linear5(h_relu))\n",
    "        y_pred = torch.tanh(self.linear6(h_relu))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model using high-fidelity alone\n",
    "## \n",
    "class DNN_HF_1(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(DNN_HF_1, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, H)\n",
    "        self.linear3 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, y_l_p,x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        new = torch.cat((y_l_p,x),1)\n",
    "        h = self.linear1(new)\n",
    "        h = self.linear2(h)\n",
    "        y_pred = self.linear3(h)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model using high-fidelity alone\n",
    "## \n",
    "class DNN_HF_2(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(DNN_HF_2, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)        \n",
    "        self.linear2 = torch.nn.Linear(H, H)\n",
    "        self.linear3 = torch.nn.Linear(H, H)\n",
    "        self.linear4 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, y_l_p,x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        new = torch.cat((y_l_p,x),1)\n",
    "        h = self.linear1(new)\n",
    "        h = self.linear2(h)\n",
    "        h = self.linear3(h)\n",
    "        y_pred = self.linear4(h)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_MF_trial(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(DNN_HF, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, H)\n",
    "        self.linear3 = torch.nn.Linear(H, H)\n",
    "        self.linear4 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        h_relu = self.linear2(h_relu).clamp(min=0)\n",
    "        h_relu = self.linear3(h_relu).clamp(min=0)\n",
    "        y_pred = self.linear4(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model using muti-fidelity alone\n",
    "## \n",
    "class DNN_MF(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(DNN_HF, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.bias1   = torch.randn((1, H))\n",
    "        self.linear2 = torch.nn.Linear(H, H)\n",
    "        self.bias2   = torch.randn((1, H))\n",
    "        self.linear3 = torch.nn.Linear(H, H)\n",
    "        self.bias3   = torch.randn((1, H))\n",
    "        self.linear4 = torch.nn.Linear(H, D_out)\n",
    "        self.bias4   = torch.randn((1, D_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = (self.linear1(x)+self.bias1).clamp(min=0)\n",
    "        h_relu = (self.linear2(h_relu)+self.bias2).clamp(min=0)\n",
    "        h_relu = (self.linear3(h_relu)+self.bias3).clamp(min=0)\n",
    "        y_pred = self.linear4(h_relu)+self.bias4\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model using muti-fidelity alone\n",
    "## \n",
    "class DNN_LF(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(DNN_LF, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, H)\n",
    "        self.linear3 = torch.nn.Linear(H, H)\n",
    "        self.linear4 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = torch.relu(self.linear1(x))\n",
    "        h_relu = torch.relu(self.linear2(h_relu))\n",
    "        h_relu = torch.relu(self.linear3(h_relu))\n",
    "        y_pred = torch.relu(self.linear4(h_relu))\n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sie] *",
   "language": "python",
   "name": "conda-env-sie-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
