{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def Y_L(x):\n",
    "#     if (x<0.5):\n",
    "#         return (0.5*(6*x-2)*(6*x-2)*math.sin(12*x-4))+(10*(x-0.5))-5\n",
    "#     else:\n",
    "#         return 3+(0.5*(6*x-2)*(6*x-2)*math.sin(12*x-4))+(10*(x-0.5))-5\n",
    "    \n",
    "    return 0.5*(6*x-2)*(6*x-2)*math.sin(12*x-4)+10*(x-0.5)-5\n",
    "def Y_H(x):\n",
    "#     if (x<0.5):\n",
    "#         return 2*Y_L(x)-20*x+20\n",
    "#     else:\n",
    "#         return 4+2*Y_L(x)-20*x+20\n",
    "    \n",
    "    return (6*x-2)*(6*x-2)*math.sin(12*x-4)\n",
    "H_x = []\n",
    "H_y = []\n",
    "H_yl = []\n",
    "for i in range(0,100,1):\n",
    "    H_x.append(i/100)\n",
    "    H_y.append(Y_H(i/100))\n",
    "    H_yl.append(Y_L(i/100))\n",
    "\n",
    "H_training_input = np.array([0.2,0.4,0.6,0.8,0.9])\n",
    "output_list = []     ## output list of high fidelity \n",
    "output_list_l = []   ## output list of low fidelity \n",
    "for i in range(0,5,1):\n",
    "    output_list.append(Y_H(H_training_input[i]))\n",
    "    output_list_l.append(Y_L(H_training_input[i]))\n",
    "H_training_output = np.asarray(output_list)\n",
    "H_training_output_l = np.asarray(output_list_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> ### (5,) (1, 5)\n"
     ]
    }
   ],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 5, 1, 20, 1\n",
    "\n",
    "H_training_input1 = H_training_input.reshape((5,1))\n",
    "H_training_input2 = H_training_input1.transpose()\n",
    "\n",
    "H_training_output1 = H_training_output.reshape((5,1))\n",
    "H_training_output2 = H_training_output1.transpose()\n",
    "\n",
    "H_training_output1_l = H_training_output_l.reshape((5,1))\n",
    "H_training_output2_l = H_training_output1_l.transpose()\n",
    "\n",
    "print(type(H_training_input),'###',H_training_input.shape,H_training_input2.shape)\n",
    "\n",
    "x1 = torch.from_numpy(H_training_input2).float()\n",
    "y1 = torch.from_numpy(H_training_output2).float()\n",
    "y1_l = torch.from_numpy(H_training_output2_l).float()\n",
    "\n",
    "x = x1.t()\n",
    "y = y1.t()\n",
    "y_l = y1_l.t()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input values tensor([[0.2000],\n",
      "        [0.4000],\n",
      "        [0.6000],\n",
      "        [0.8000],\n",
      "        [0.9000]])\n",
      "input values tensor([[-0.6397],\n",
      "        [ 0.1148],\n",
      "        [-0.1494],\n",
      "        [-4.9491],\n",
      "        [ 5.7120]])\n",
      "input values tensor([[-8.3199],\n",
      "        [-5.9426],\n",
      "        [-4.0747],\n",
      "        [-4.4746],\n",
      "        [ 1.8560]])\n"
     ]
    }
   ],
   "source": [
    "print(\"input values\",x)\n",
    "print(\"input values\",y)\n",
    "print(\"input values\",y_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch model\n",
    "\n",
    "%run models_for_multi.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DNN_LF(D_in, H, D_out)\n",
    "model2 = multi_fnn(D_in, H, D_out)\n",
    "\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "error = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.LBFGS(model1.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(100):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model1(x)\n",
    "        loss = error(output, y_l)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    #print(optimizer.step(closure))\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3137],\n",
      "        [-0.3286],\n",
      "        [-0.3403],\n",
      "        [-0.3489],\n",
      "        [-0.3521]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'err' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-8d0edf199cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss_yh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0moptimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-161-8d0edf199cb3>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_h\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_l_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss_yh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#loss = loss_yl + loss_yh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'err' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer2 = torch.optim.LBFGS(model2.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(100):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        y_l_p   = model1(x)\n",
    "        y_h     = model2(y_l_p,x)\n",
    "        print(y_l_p)\n",
    "        print(y_h)\n",
    "        err \n",
    "        loss_yh = error(y_h, y)\n",
    "        #loss = loss_yl + loss_yh\n",
    "        loss_yh.backward()\n",
    "        return loss\n",
    "    optimizer2.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.asarray(H_x)\n",
    "temp1 = temp.reshape((100,1))\n",
    "temp2 = temp1.transpose()\n",
    "t1 = torch.FloatTensor(temp2)\n",
    "t = t1.t()\n",
    "\n",
    "Ttemp  = np.asarray(H_x)\n",
    "Ttemp1 = Ttemp.reshape((100,1))\n",
    "Ttemp2 = Ttemp1.transpose()\n",
    "Tt1    = torch.FloatTensor(Ttemp2)\n",
    "Tt     = Tt1.t()\n",
    "\n",
    "\n",
    "p_l = model1(t)\n",
    "p_h_1 = model2(p_l,t)\n",
    "p_h_2 = model3(p_l,t)\n",
    "# p_h_1 = model2(Tt,t)\n",
    "# p_h_2 = model3(Tt,t)\n",
    "prediction = alpha * p_h_1 + (1-alpha)*p_h_2\n",
    "\n",
    "plt.plot(H_x,H_y,H_training_input,H_training_output,'ro',H_x,prediction.tolist())\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
